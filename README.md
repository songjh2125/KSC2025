# ðŸ§  Transformer-XL Topic-Aware (TXL-TA)

ì´ í”„ë¡œì íŠ¸ëŠ” **Transformer-XL (TXL)**ì„ ê¸°ë°˜ìœ¼ë¡œ **Topic-Aware Memory êµ¬ì¡°**ë¥¼ ì‹¤í—˜ì ìœ¼ë¡œ í™•ìž¥í•œ ì½”ë“œë² ì´ìŠ¤ìž…ë‹ˆë‹¤.  
Hugging Face Datasets ë° Tokenizersë¥¼ ì´ìš©í•´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ë‹¨ìˆœí™”í•˜ê³ ,  
ì„¸ì…˜ ë‹¨ìœ„ ê¸°ì–µ ìœ ì§€(Recall) ë° ë§ê°(Forgetting) ê³¡ì„ ì„ í‰ê°€í•  ìˆ˜ ìžˆë„ë¡ êµ¬ì„±ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.

---

## ðŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°
```plaintext
transformer-xl/
â”œâ”€ txl/
â”‚  â”œâ”€ mem_transformer.py     # ì›ë³¸ ê·¸ëŒ€ë¡œ(ë³µì‚¬ë³¸). ìˆ˜ì • ê¸ˆì§€
â”‚  â”œâ”€ mem_transformer_ta.py  # Topic-Aware ìˆ˜ì •ë³¸
â”‚  â””â”€ __init__.py
â”‚
â”œâ”€ txl_hf/
â”‚  â”œâ”€ train_hf.py            # HF Trainer ê¸°ë°˜ í•™ìŠµ/í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â”‚  â”œâ”€ collator_stream.py     # ìŠ¤íŠ¸ë¦¬ë° Collator (ì„¸ì…˜ ë‹¨ìœ„ ì²˜ë¦¬)
â”‚  â”œâ”€ build_dataset.py       # HF Dataset ìƒì„± ë° ì „ì²˜ë¦¬
â”‚  â”œâ”€ build_tokenizer.py     # BPE ê¸°ë°˜ Tokenizer ë¹Œë“œ
â”‚  â”œâ”€ utils_logging.py       # ë¡œê·¸ ë° ì²´í¬í¬ì¸íŠ¸ ìœ í‹¸ë¦¬í‹°
â”‚  â”œâ”€ mem_baseline.py        # HFìš© TXL ëž˜í¼ (baseline)
â”‚  â”œâ”€ mem_ta.py              # Topic-Aware Memory ì ìš© HF ëž˜í¼
â”‚  â””â”€ __init__.py
â”‚
â”œâ”€ data/
â”‚  â”œâ”€ aihub/                 # AI Hub ì¼ìƒëŒ€í™” ë°ì´í„°ì…‹
â”‚  â”œâ”€ kowiki/                # í•œêµ­ì–´ Wikipedia ë¬¸ì„œ ë°ì´í„°
â”‚  â””â”€ kodial/                # Ko-Dial (KoNLP ê³µê°œ ëŒ€í™” corpus)
â”‚
â”œâ”€ artifacts/
â”‚  â””â”€ tokenizer/
â”‚     â””â”€ ko_bpe.json         # í•™ìŠµìš© BPE í† í¬ë‚˜ì´ì € ì €ìž¥ íŒŒì¼
â”‚
â””â”€ logs/                     # í•™ìŠµ ë° í‰ê°€ ë¡œê·¸ ì €ìž¥ ê²½ë¡œ
```

---

## âš™ï¸ ì£¼ìš” êµ¬ì„± ìš”ì†Œ ì„¤ëª…

| ëª¨ë“ˆ | ì—­í•  |
|------|------|
| `txl/mem_transformer.py` | Transformer-XL ì›ë³¸ (Google ê³µì‹ êµ¬í˜„ ê¸°ë°˜) |
| `txl/mem_transformer_ta.py` | Topic-Aware Memory êµ¬ì¡° ì¶”ê°€ ë²„ì „ |
| `txl_hf/train_hf.py` | Hugging Face Trainer ê¸°ë°˜ í•™ìŠµ ë£¨í”„ |
| `txl_hf/mem_baseline.py` | HF í¬ë§·ìš© Transformer-XL ëž˜í¼ |
| `txl_hf/mem_ta.py` | Topic-Aware Memory ì ìš© HF ëž˜í¼ |
| `txl_hf/build_dataset.py` | ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ë° HF Dataset ê°ì²´ ìƒì„± |
| `txl_hf/build_tokenizer.py` | BPE ê¸°ë°˜ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ë¹Œë“œ |
| `txl_hf/utils_logging.py` | í•™ìŠµ ë¡œê·¸, ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ |
| `txl_hf/collator_stream.py` | ì„¸ì…˜ ë‹¨ìœ„ë¡œ ì‹œí€€ìŠ¤ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ |

---

## ðŸš€ ì‹¤í–‰ ì˜ˆì‹œ

1ï¸âƒ£ ë°ì´í„° ì „ì²˜ë¦¬

```bash
python txl_hf/build_dataset.py --source aihub
```

2ï¸âƒ£ í† í¬ë‚˜ì´ì € ìƒì„±
```bash
python txl_hf/build_tokenizer.py \
  --data_dir data/aihub \
  --output_dir artifacts/tokenizer
```

3ï¸âƒ£ í•™ìŠµ (Baseline TXL)
```bash
python txl_hf/train_hf.py \
  --model mem_baseline \
  --dataset aihub \
  --batch_size 4 \
  --lr 1e-4 \
  --epochs 5
```

4ï¸âƒ£ í•™ìŠµ (Topic-Aware TXL)
```bash
python txl_hf/train_hf.py \
  --model mem_ta \
  --dataset aihub \
  --batch_size 4 \
  --lr 1e-4 \
  --epochs 5
```

5ï¸âƒ£ ë¡œê·¸ ë° ì‹œê°í™”
```bash
tensorboard --logdir logs/
```

## ðŸ§© Topic-Aware Memory êµ¬ì¡° ìš”ì•½

| í•­ëª© | Transformer-XL (TXL) | Topic-Aware TXL (TA-TXL) |
|------|-----------------------|---------------------------|
| Memory ì „ë‹¬ | ëª¨ë“  ì‹œí€€ìŠ¤ì˜ hidden stateë¥¼ ë‹¨ìˆœížˆ ì´ì–´ë¶™ìž„ | ì¤‘ìš” topic memoryë§Œ ì„ ë³„ ìœ ì§€ |
| ê¸°ì–µ ë‹¨ìœ„ | ì‹œí€€ìŠ¤ (sequence) | ì„¸ì…˜ + í† í”½ ë‹¨ìœ„ |
| ê°±ì‹  ì‹œì  | ì‹œí€€ìŠ¤ ë‹¨ìœ„ (ë§¤ batch) | í† í”½ ê²½ê³„ ê°ì§€ ì‹œ selective update |
| ëª©ì  | ë¬¸ë§¥ ê¸¸ì´ í™•ìž¥ | ì£¼ì œ ì§€ì†ì„± ìœ ì§€ ë° ë§ê° ì œì–´ |

---

## ðŸ§  êµ¬ì¡° ê°œë…ë„ (Mermaid)
```mermaid
flowchart LR

%% =========================
%% RMT â€” Sequential memory passing
%% =========================
subgraph RMT[RMT â€” ìˆœì°¨ ë©”ëª¨ë¦¬ ì „ë‹¬]
  direction LR
  R1[Segment 1] --> RM1[Memory_1]
  R2[Segment 2 + Memory_1] --> RM2[Memory_2]
  R3[Segment 3 + Memory_2] --> RM3[Memory_3]
  Rellipsis[(...)] --> RMellipsis[(...)]
end

%% =========================
%% Memformer â€” Global memory update
%% =========================
subgraph MEM[Memformer â€” Global Memory Update]
  direction TB
  M1[Segment 1] --> GMU[Global Memory Update]
  M2[Segment 2] --> GMU
  M3[Segment 3] --> GMU
  GMU --> GM[Global Memory]
end

%% =========================
%% HMT --- Sensory --> Short --> Long (periodic accumulation)
%% =========================
subgraph HMT[HMT --- ê°ê°â†’ë‹¨ê¸°â†’ìž¥ê¸°(ì£¼ê¸°ì  ëˆ„ì )]
  direction TB
  H1S[Segment 1] --> H1x[sensory_1] --> H1s[short_memory_1] --> HL[long_memory]
  H2S[Segment 2] --> H2x[sensory_2] --> H2s[short_memory_2] --> HL
  H3S[Segment 3] --> H3x[sensory_3] --> H3s[short_memory_3] --> HL
  H4S[Segment 4] --> H4x[sensory_4] --> H4s[short_memory_4] --> HL
end

%% =========================
%% Ours â€” Topic-aware summary & selective reuse
%% =========================
subgraph OURS[Ours â€” í† í”½ ìš”ì•½ + ì„ íƒì  ì°¸ì¡°(routing)]
  direction TB
  O1[Sequence 1] --> O1m[seq_memory_1]
  O2[Sequence 2] --> O2m[seq_memory_2]
  O3[Sequence 3] --> O3m[seq_memory_3]

  %% Topic Block A summary â†’ topic_memory_A
  O1m --> TBA[Topic Block A ìš”ì•½]
  O2m --> TBA
  O3m --> TBA
  TBA --> TMA[topic_memory_A]

  NewTopic[(ì‚¬ìš©ìžê°€ ìƒˆë¡œìš´ í™”ì œ ì œì‹œ)]
  NewTopic --> O4[Sequence 4]
  O4 --> O4m[seq_memory_4]

  %% Selective reuse (only relevant topic memory)
  TMA -. ì„ íƒì  ì°¸ì¡° .-> O4

  O5[Sequence 5] --> O5m[seq_memory_5]
end

%% =========================
%% Legend
%% =========================
classDef note fill:#f7f7f7,stroke:#aaa,color:#333,font-size:11px;
```
